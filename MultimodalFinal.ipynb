{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brandon-Yip2/AndroidStudioProjectCollection/blob/main/MultimodalFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing for text"
      ],
      "metadata": {
        "id": "pWHEfuO6S8SA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os"
      ],
      "metadata": {
        "id": "0aHEEaVrTUbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "xkS8Cf9Zcnpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888621d8-013f-429d-83f0-609fc4785c5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip Episode_transcripts.zip -d EpisodeTranscripts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yE0UdELcLt2r",
        "outputId": "98cc18c2-8f90-4e50-a118-76b63089c9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open Episode_transcripts.zip, Episode_transcripts.zip.zip or Episode_transcripts.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPH8xiW4yH1n",
        "outputId": "9b7c30f4-2f55-4f05-d8c5-170c2f48458e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "kyWtSGuvOOSe",
        "outputId": "0735391c-683f-4a19-f7ae-32a85ba7fe1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'EpisodeTranscripts/Episode transcipts'\n",
            "/content\n",
            "/content\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '01a.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f19fe8aea753>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepisode_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mfile_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '01a.txt'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import contractions\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "\n",
        "%cd EpisodeTranscripts/Episode\\ transcipts\n",
        "\n",
        "episode_list = [\"01a.txt\", \"01c.txt\", \"02a.txt\", \"02b.txt\", \"03a.txt\", \"03b.txt\",\n",
        "                \"04a.txt\", \"04b.txt\", \"05a.txt\", \"05b.txt\", \"06a.txt\", \"06b.txt\",\n",
        "                \"07a.txt\", \"07b.txt\", \"08a.txt\", \"08b.txt\", \"09a.txt\", \"09b.txt\",\n",
        "                \"10a.txt\", \"10b.txt\", \"11a.txt\", \"11b.txt\", \"12a.txt\", \"12b.txt\",\n",
        "                \"13a.txt\", \"13b.txt\", \"14a.txt\", \"14b.txt\", \"15a.txt\", \"15b.txt\",\n",
        "                \"16a.txt\", \"16b.txt\", \"17a.txt\", \"17b.txt\", \"18a.txt\", \"18b.txt\",\n",
        "                \"19a.txt\", \"19b.txt\", \"20a.txt\", \"20b.txt\"\n",
        "                ]\n",
        "cols = ['text', 'words', 'id']\n",
        "lst = []\n",
        "index = 0\n",
        "print(os.getcwd())\n",
        "\n",
        "for filename in episode_list:\n",
        "  f = open(filename, 'r')\n",
        "\n",
        "  file_contents = f.read()\n",
        "\n",
        "  # print (file_contents)\n",
        "  # nltk.download('stopwords')\n",
        "\n",
        "\n",
        "  f.close()\n",
        "\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "  # Expand contractions\n",
        "  # file_contents = file_contents.apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
        "  file_contents = file_contents.lower()\n",
        "  file_contents = contractions.fix(file_contents)\n",
        "\n",
        "\n",
        "  file_contents = re.sub(\"[\\n]\", ' ', file_contents)\n",
        "  file_contents = re.sub(\"[^ 0-9a-zA-Z]\", '', file_contents)\n",
        "  # print(file_contents)\n",
        "\n",
        "  file_contents = [w for w in file_contents.split() if not w in stopwords.words('english')]\n",
        "  # print(file_contents)\n",
        "\n",
        "  # Stemming text (reduce vocabulary size, let model treat word variations as same word)\n",
        "  # the stemmer requires a language parameter\n",
        "  snow_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "  #stem's of each word\n",
        "  stem_words = []\n",
        "  for w in file_contents:\n",
        "      x = snow_stemmer.stem(w)\n",
        "      stem_words.append(x)\n",
        "\n",
        "  # print(stem_words)\n",
        "  # #print stemming results\n",
        "  # for e1,e2 in zip(file_contents,stem_words):\n",
        "  #     print(e1+' ----> '+e2)\n",
        "\n",
        "  lst.append([filename, stem_words, index])\n",
        "  index = index+1;\n",
        "\n",
        "df1 = pd.DataFrame(lst, columns=cols)\n",
        "print(df1)\n",
        "\n",
        "%cd ../..\n",
        "testlst = []\n",
        "#Get the words detected from the audio files\n",
        "df_ReadAudio = pd.read_csv('normalTranscripts.csv')\n",
        "\n",
        "for ind in df_ReadAudio.index:\n",
        "\n",
        "    test_contents = df_ReadAudio['text'][ind]\n",
        "    test_contents = test_contents.lower()\n",
        "    test_contents = contractions.fix(test_contents)\n",
        "    test_contents = re.sub(\"[\\n]\", ' ', test_contents)\n",
        "    test_contents = re.sub(\"[^ 0-9a-zA-Z]\", '', test_contents)\n",
        "    test_contents = [w for w in test_contents.split() if not w in stopwords.words('english')]\n",
        "    test_stem_words = []\n",
        "    for w in test_contents:\n",
        "        x = snow_stemmer.stem(w)\n",
        "        test_stem_words.append(x)\n",
        "\n",
        "\n",
        "    col = [\"testwords\", \"id\"]\n",
        "    testlst.append([test_stem_words, df_ReadAudio['episodeNumber'][ind]])\n",
        "    df2 = pd.DataFrame(testlst, columns=col)\n",
        "\n",
        "print(df2)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction for texts"
      ],
      "metadata": {
        "id": "ut1APTxigsfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "GXMaI5UAgqgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1[\"words\"]"
      ],
      "metadata": {
        "id": "av9ud4I9nQSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get word counts in data\n",
        "word2count = {}\n",
        "\n",
        "for row in df1[\"words\"]:\n",
        "    for word in row:\n",
        "        if word not in word2count.keys():\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "\n",
        "# Constrain word count to top 1000 most popular words\n",
        "# word2count_frequent = sorted(word2count.items(),  key=lambda x:x[1], reverse=True)[0:1000]\n",
        "# Drop word counts from result\n",
        "# unique_words = [i[0] for i in word2count_frequent]\n",
        "# unique_words = list(word2count.keys())\n",
        "print(len(word2count))\n"
      ],
      "metadata": {
        "id": "6pb06uOZpOGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF Vectorizer requires dummy function for tokenizer and preprocessor because text is already tokenized (just return the input)\n",
        "def dummy_fun(doc):\n",
        "    return doc\n",
        "\n",
        "# Compute TF-IDF\n",
        "tr_idf_model = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer=dummy_fun,\n",
        "    preprocessor=dummy_fun,\n",
        "    token_pattern=None,\n",
        "    max_features=1000) # Take top 1000 features\n",
        "\n",
        "tf_idf_vector = tr_idf_model.fit_transform(df1[\"words\"])\n",
        "tf_idf_array = tf_idf_vector.toarray()\n",
        "\n",
        "# Use training data vocabulary for test data vectorizer so that word features are in the same position\n",
        "tr_idf_model_test = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer=dummy_fun,\n",
        "    preprocessor=dummy_fun,\n",
        "    token_pattern=None,\n",
        "    vocabulary=tr_idf_model.get_feature_names_out()) # use vocabulary from training data\n",
        "\n",
        "\n",
        "tf_idf_vector_test = tr_idf_model_test.fit_transform(df2[\"testwords\"])\n",
        "tf_idf_array_test = tf_idf_vector_test.toarray()\n",
        "\n",
        "print(tf_idf_array.shape)\n",
        "print(len(tf_idf_array[0]))\n",
        "\n",
        "print(tf_idf_array_test.shape)\n",
        "print(tf_idf_array_test[0])"
      ],
      "metadata": {
        "id": "aAYIGJ4JnB7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, roc_curve, auc, classification_report"
      ],
      "metadata": {
        "id": "hjL8VgH6uVIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all my classifiers\n",
        "lc = LogisticRegression(max_iter=1000)\n",
        "svc = SVC(probability=True)\n",
        "nbc = GaussianNB()\n",
        "rfc = RandomForestClassifier()"
      ],
      "metadata": {
        "id": "dkE1hVaAuX3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output is sentiment values\n",
        "y_train = df1['id'].to_numpy()\n",
        "y_test = df2['id'].to_numpy()\n",
        "\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# # Remove the mean and scale to unit variance\n",
        "# scale = StandardScaler()\n",
        "# # x can be chosen from bag of words, TF-IDF, word2vec\n",
        "# # x_train = scale.fit_transform(bag_of_words)\n",
        "# x_train = scale.fit_transform(tf_idf_array)\n",
        "# # x_train = scale.fit_transform(w2v)\n",
        "\n",
        "# # x_test = scale.fit_transform(bag_of_words_test)\n",
        "# x_test = scale.fit_transform(tf_idf_array_test)\n",
        "# # x_test = scale.fit_transform(w2v_test)\n",
        "\n",
        "x_train = tf_idf_array\n",
        "x_test = tf_idf_array_test"
      ],
      "metadata": {
        "id": "-6RAxJl4ubU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass my training dataset into model.fit() to train my classifiers\n",
        "lc.fit(x_train, y_train)\n",
        "nbc.fit(x_train, y_train)\n",
        "rfc.fit(x_train, y_train)\n",
        "svc.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "EbC3dO6WvS1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run model on test dataset\n",
        "y_lc_predicted = lc.predict(x_test)         # Gives predicted episode number\n",
        "y_lc_pred_proba = lc.predict_proba(x_test)  # Gives probabilities for each episode number in array form\n",
        "\n",
        "y_nbc_predicted = nbc.predict(x_test)\n",
        "y_nbc_pred_proba = nbc.predict_proba(x_test)\n",
        "\n",
        "y_rfc_predicted = rfc.predict(x_test)\n",
        "y_rfc_pred_proba = rfc.predict_proba(x_test)\n",
        "\n",
        "y_svc_predicted = svc.predict(x_test)         # Gives predicted episode number\n",
        "y_svc_pred_proba = svc.predict_proba(x_test)\n"
      ],
      "metadata": {
        "id": "FJxLzU5EvYd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "id": "kjHLv7Wbxull"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1)\n",
        "print(df2)"
      ],
      "metadata": {
        "id": "Y02v6kXKxaqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ep_dict = {\n",
        "  0: \"1a: Help Wanted\",\n",
        "  1: \"1c: Tea at the Treedome\",\n",
        "  2: \"2a: Bubblestand\",\n",
        "  3: \"2b: Ripped Pants\",\n",
        "  4: \"3a: Jellyfishing\",\n",
        "  5: \"3b: Plankton!\",\n",
        "  6: \"04a: Naughty Nautical Neighbors\",\n",
        "  7: \"04b: Boating School\",\n",
        "  8: \"05a: Pizza Delivery\",\n",
        "  9: \"05b: Home Sweet Pineapple\",\n",
        "  10: \"06a: Mermaid Man and Barnacle Boy\",\n",
        "  11: \"06b: Pickles\",\n",
        "  12: \"07a: Hall Monitor\",\n",
        "  13: \"07b: Jellyfish Jam\",\n",
        "  14: \"08a: Sandy's Rocket\",\n",
        "  15: \"08b: Squeaky Boots\",\n",
        "  16: \"09a: Nature Pants\",\n",
        "  17: \"09b: Opposite Day\",\n",
        "  18: \"10a: Culture Shock\",\n",
        "  19: \"10b: F.U.N\",\n",
        "\n",
        "  20: \"11a: MuscleBob BuffPants\",\n",
        "  21: \"11b: Squidward the Unfriendly Ghost\",\n",
        "  22: \"12a: The Chaperone\",\n",
        "  23: \"12b: Employee of the Month\",\n",
        "  24: \"13a: Scaredy Pants\",\n",
        "  25: \"13b: I Was a Teenage Gary\",\n",
        "  26: \"14a: SB-129\",\n",
        "  27: \"14b: Karate Choppers\",\n",
        "  28: \"15a: Sleepy Time\",\n",
        "  29: \"15b: Suds\",\n",
        "  30: \"16a: Valentine's Day\",\n",
        "  31: \"16b: The Paper\",\n",
        "  32: \"17a: Arrgh!\",\n",
        "  33: \"17b: Rock Bottom\",\n",
        "  34: \"18a: Texas\",\n",
        "  35: \"18b: Walking Small\",\n",
        "  36: \"19a: Fools in April\",\n",
        "  37: \"19b: Neptune's Spatula\",\n",
        "  38: \"20a: Hooky\",\n",
        "  39: \"20b: Mermaid Man and Barnacle Boy II\",\n",
        "\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "h89X1wfw21zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_test)\n",
        "print(y_lc_predicted)\n",
        "print(y_lc_pred_proba)\n",
        "print(classification_report(y_test, y_lc_predicted))\n",
        "\n",
        "print(y_nbc_predicted)\n",
        "print(y_nbc_pred_proba)\n",
        "print(classification_report(y_test, y_nbc_predicted))\n",
        "\n",
        "print(y_rfc_predicted)\n",
        "print(y_rfc_pred_proba)\n",
        "print(classification_report(y_test, y_rfc_predicted))\n",
        "\n",
        "print(y_test)\n",
        "print(y_svc_predicted)\n",
        "print(y_svc_pred_proba)\n",
        "print(classification_report(y_test, y_svc_predicted))\n"
      ],
      "metadata": {
        "id": "YAEQ2JEzvd9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def Matrix(trueY, predictedY):\n",
        "  #\n",
        "  # Calculate the confusion matrix\n",
        "  #\n",
        "  conf_matrix = confusion_matrix(y_true=trueY, y_pred=predictedY)\n",
        "  #\n",
        "  # Print the confusion matrix using Matplotlib\n",
        "  #\n",
        "  fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
        "  ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
        "  for i in range(conf_matrix.shape[0]):\n",
        "      for j in range(conf_matrix.shape[1]):\n",
        "          ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
        "\n",
        "  plt.xlabel('Predictions', fontsize=18)\n",
        "  plt.ylabel('Actuals', fontsize=18)\n",
        "  plt.title('Confusion Matrix', fontsize=18)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "4lk1Iwm1bxkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Matrix(y_test, y_lc_predicted)\n",
        "Matrix(y_test, y_nbc_predicted)\n",
        "Matrix(y_test, y_rfc_predicted)\n",
        "Matrix(y_test, y_svc_predicted)\n",
        "\n"
      ],
      "metadata": {
        "id": "rdCI0nSvcVqn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}